a="I will walk 500 miles everyday and wlak 500 miles more than previous miles."

!pip install -q wordcloud
import wordcloud
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

# tokenization
from nltk import word_tokenize,sent_tokenize
print("Tokenized words:",word_tokenize(a))
print("Tokenized sentence:",sent_tokenize(a))

# pos tagging
from nltk import pos_tag
token=word_tokenize(a)
tagged=pos_tag(token)
print(tagged)

# stopwords
from nltk.corpus import stopwords
words=stopwords.words('english')
cleaned=[]
for word in token:
 if word not in words:
 cleaned.append(word)
print(cleaned)

# stemmer
from nltk.stem import PorterStemmer
stemm=PorterStemmer()
stemmed=[stemm.stem(word)for word in token]
print(" ".join(stemmed))

# lemmatization
from nltk.stem import WordNetLemmatizer
lem=WordNetLemmatizer()
ans=[lem.lemmatize(word)for word in token]
print(" ".join(ans))

from nltk.probability import FreqDist
fdist=FreqDist(token)
print(fdist)
fdist.most_common(2)

import matplotlib.pyplot as plt
fdist.plot(30,cumulative=False)
plt.show()
